# ‚úàÔ∏è Projeto: An√°lise e Previs√£o de **Load Factor** (ANAC)

Este reposit√≥rio re√∫ne um fluxo de **ETL + EDA + Modelagem** para analisar e projetar o **load factor** (taxa de ocupa√ß√£o de assentos) da avia√ß√£o comercial brasileira a partir dos **microdados da ANAC**, cobrindo o per√≠odo de **jan/2023 a jul/2025**.

> Arquivo principal do projeto: **`projeto_machine_learning_load_factor.ipynb`**

---

## üéØ Objetivo

* Consolidar e limpar os microdados oficiais da ANAC.
* Calcular o **load factor di√°rio/mensal** por companhia a√©rea e por recorte de neg√≥cio (ex.: REGULAR / DOM√âSTICA).
* Explorar padr√µes sazonais (por m√™s e por dia da semana) e tend√™ncias.
* Treinar modelos de **regress√£o** (Spark MLlib) para **prever o load factor** nos meses seguintes.
* Compara√ß√µes entre empresas **AD (Azul), G3 (Gol) e JJ (Latam)**  

---

## üìä Fonte dos Dados

Os dados utilizados s√£o da base **B√°sica** disponibilizada pela ANAC:  
üëâ [Microdados da Avia√ß√£o Comercial - ANAC](https://www.gov.br/anac/pt-br/assuntos/regulados/empresas-aereas/Instrucoes-para-a-elaboracao-e-apresentacao-das-demonstracoes-contabeis/envio-de-informacoes/microdados)

- Per√≠odo considerado: **janeiro/2023 at√© julho/2025**  
- Colunas utilizadas incluem:
  - Companhia a√©rea (`sg_empresa_iata`)
  - Tipo de voo (`ds_natureza_etapa`)
  - Grupo de voo (`ds_grupo_di`)
  - N√∫mero de passageiros pagos/gr√°tis
  - Assentos ofertados
  - Tipo de servi√ßo (excluindo cargueiro)


## üõ†Ô∏è Tecnologias Utilizadas

- **PySpark** ‚Üí leitura, tratamento e transforma√ß√£o dos dados em larga escala  
- **Pandas** ‚Üí integra√ß√£o e manipula√ß√£o de subconjuntos de dados  
- **Matplotlib** ‚Üí visualiza√ß√µes (s√©ries temporais, gr√°ficos comparativos)  
- **Python** (Jupyter Notebook)  

## üìÇ Estrutura do Reposit√≥rio
```
load_factor_anac/
‚îÇ‚îÄ‚îÄ projeto_machine_learning_load_factor.ipynb # Notebook principal com ETL e an√°lises
‚îÇ‚îÄ‚îÄ data/ # (opcional) pasta para armazenamento local de arquivos
‚îÇ‚îÄ‚îÄ README.md # Este arquivo
‚îÇ‚îÄ‚îÄ requirements.txt
```

## ‚öôÔ∏è Ambiente e requisitos

O notebook utiliza **PySpark** para engenharia de atributos e modelagem, al√©m de bibliotecas Python usuais.

### Requisitos m√≠nimos

* **Python 3.10+**
* **Java 11+** (necess√°rio para PySpark)
* Bibliotecas: `pyspark`, `pandas`, `numpy`, `matplotlib`

### Exemplo de `requirements.txt`

```txt
pyspark>=3.5
pandas>=2.2
numpy>=1.26
matplotlib>=3.8
jupyter>=1.0  # opcional, para executar localmente
ipykernel>=6  # opcional, para registrar o kernel do projeto
```

### Configura√ß√£o r√°pida (Linux/macOS)

```bash
# 1) Crie e ative um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2) Instale as depend√™ncias
pip install -r requirements.txt

# 3) Garanta o Java instalado e exporte JAVA_HOME, se necess√°rio
# Exemplo (ajuste conforme seu sistema):
export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"

# 4) Rode o Jupyter e abra o notebook
jupyter notebook notebooks/projeto_machine_learning_load_factor.ipynb
```

> Caso utilize **VS Code**, instale a extens√£o **Jupyter** e selecione o kernel do ambiente virtual.

---


## üßπ ETL (resumo)

1. **Leitura** dos CSV/ZIP dos microdados.
2. **Sele√ß√£o de recortes** (ex.: `ds_grupo_di == "REGULAR"`, `ds_natureza_etapa == "DOM√âSTICA"`, empresas de interesse como `AD`, `G3`, `JJ`).
3. **Limpeza** (remo√ß√£o de colunas vazias/irrelevantes, padroniza√ß√£o de datas e tipos).
4. **Features**:

   * `load_factor_dia` por voo/data (raz√£o entre passageiros e assentos ofertados).
   * Agrega√ß√µes **di√°rias** e **mensais** por companhia.

> Os passos est√£o implementados e comentados diretamente no notebook.

---

## üîé EDA (explora√ß√£o)

* Evolu√ß√£o **mensal** do load factor por companhia.
* Comportamento **por dia da semana**.
* Identifica√ß√£o de sazonalidade e outliers.

Gr√°ficos t√≠picos:

* Linha: **Evolu√ß√£o mensal** do `load_factor` (ex.: `plt.plot` por companhia).
* Barras: **Distribui√ß√£o por dia da semana**.

---

## ü§ñ Modelagem (Spark MLlib)

Modelos de regress√£o utilizados no notebook:

* **LinearRegression**
* **RandomForestRegressor**
* **GBTRegressor**

Pipeline geral:

1. Montagem de vetores com **`VectorAssembler`**.
2. Treino/valida√ß√£o com **`RegressionEvaluator`**.
3. **Previs√µes** para a janela de interesse (ex.: pr√≥ximos 3 meses) e compara√ß√£o por companhia a√©rea.

> Ajuste hiperpar√¢metros, janelas e vari√°veis explicativas conforme sua necessidade.

---

## ‚ñ∂Ô∏è Como executar o projeto

1. **Baixe** os microdados da ANAC e **coloque em `data/raw/`**.
2. **Execute** o notebook `notebooks/projeto_machine_learning_load_factor.ipynb` (ou na raiz, conforme seu layout) e siga as c√©lulas na ordem.
3. **Salve** as sa√≠das (tabelas/figuras) em `data/processed/` e `reports/figures/`, se desejar organizar resultados.

---

## üìà Exemplos de resultados

* Curvas do **load factor mensal** por companhia (2023‚Äë01 ‚Üí 2025‚Äë07).
* **Tabela/CSV** com previs√µes por data/companhia.
* Gr√°ficos de **import√¢ncia de features** nos modelos de √°rvore (quando aplic√°vel).

> Os exemplos s√£o gerados durante a execu√ß√£o do notebook e podem variar conforme o per√≠odo/dados carregados.

---

## üß© Troubleshooting

### Erro: `PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.`

#### üí° Contexto
Durante a cria√ß√£o da `SparkSession` no Jupyter Notebook (ou Google Colab), o PySpark apresentou o erro acima, impossibilitando a inicializa√ß√£o da sess√£o.  
O motivo foi a aus√™ncia do **Java Runtime Environment (JRE)** no ambiente, requisito fundamental para o PySpark, pois o Spark roda sobre uma JVM (Java Virtual Machine).

#### üß† Causa raiz
O PySpark depende do **Java Gateway** para comunicar o interpretador Python com o n√∫cleo Java do Apache Spark.  
Sem o Java instalado ou com a vari√°vel de ambiente `JAVA_HOME` ausente/incorreta, o gateway falha antes de estabelecer a comunica√ß√£o, gerando o erro.

#### üõ†Ô∏è Solu√ß√£o aplicada
1. Instalar o Java 17 (OpenJDK):
   ```bash
   !apt-get update -y
   !apt-get install -y openjdk-17-jre-headless

2. Configurar as vari√°veis de ambiente:
   ```bash
   import os

   os.environ.pop("SPARK_HOME", None)  # remove SPARK_HOME se estiver setada incorretamente
   os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"
   os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

3. Verificar a instala√ß√£o:
   ```bash
   !java -version

4. Recriar a sess√£o Spark:
   ```bash
   from pyspark.sql import SparkSession

   sessao_spark = (SparkSession.builder
                    .master("local[*]")
                    .appName("Load Factor (ANAC)")
                    .getOrCreate())

### ‚úÖ Resultado

Ap√≥s configurar o Java corretamente e limpar a vari√°vel SPARK_HOME, a sess√£o Spark foi criada com sucesso.
Isso garante que o ambiente PySpark possa ser utilizado normalmente para leitura, processamento e an√°lise dos dados ANAC.

---

## üìù Notas e boas pr√°ticas

* **Datas faltantes**: verifique lacunas e trate-as antes de treinar modelos.
* **Qualidade dos dados**: cheque valores zerados (ex.: passageiros pagos/gr√°tis) e registros de carga quando n√£o deseja inclu√≠-los.
* **Reprodutibilidade**: fixe seeds quando comparar modelos.

---

## üìö Cr√©ditos

* **Dados**: Ag√™ncia Nacional de Avia√ß√£o Civil (ANAC) ‚Äì microdados p√∫blicos.
* **Projeto e estudo**: este reposit√≥rio foi criado para estudo/aprendizado em ci√™ncia de dados e engenharia de dados com foco em s√©ries temporais/ML para **load factor**.

---



## üí¨ Contato

Sinta-se √† vontade para abrir **Issues** ou **Pull Requests** com sugest√µes de melhoria, corre√ß√µes e novas an√°lises.

